\documentclass{beamer}
\input{BeamOptions.tex}
\begin{document}

<<setup, include=FALSE>>=
options(replace.assign=TRUE, width=40)
opts_knit$set(progress=FALSE)
library(ggplot2)
library(dplyr)
@

\title{Transformations}
\institute{CSU Chico, Math 314}
\date{\today}
\maketitle

\frame {\frametitle{outline}
  \tableofcontents
}

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{outline}
    \tableofcontents[currentsection]
  \end{frame}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% frames %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Recap}

\begin{frame}
  \frametitle{Recap, statistical methods assume}
So far we have covered statistical methods each of which has some set of given assumptions.  Sometimes nonparametric methods provide alternative means to test our hypotheses of interest, sometimes we need a less extreme solution.  For instance, some assumptions seem reasonable, but maybe not all of them.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Transformations, motivation}
Consider the data set \texttt{ape::carnivora}.  
<<>>=
suppressMessages({library(ape)
    library(ggplot2)
    data(carnivora)})
p1 <- qplot(BW, SW, data=carnivora,
          na.rm=TRUE, xlab="Birth weight (g)",
          ylab="Body weight (kg)")
@ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Transformations, motivation}
The variables just don't seem appropriately linear.
<<echo=FALSE, fig.height=3, fig.width=3, fig.align="center">>=
p1
@ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Transformations, motivation}
If we fit linear regression to these data, nothing looks good.
<<>>=
fit_ew <- lm(SW\textasciitildeBW, data=carnivora)
# summary(fit_ew) # RStudio
r <- rstandard(fit_ew)
yhat <- fitted(fit_ew)
@ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Transformations, motivation}
<<echo=FALSE, fig.height=3, fig.width=3, fig.align="center">>=
qplot(yhat, r) + geom_hline(yintercept=0)
@ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Transformations, motivation}
<<echo=FALSE, fig.height=3, fig.width=3, fig.align="center">>=
qplot(r, geom="histogram", binwidth=1/3)
@ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Transformations, motivation}
  So what is the problem?  Largely, it's due to the extreme right skew of the data.  
<<>>=
pSW <- qplot(SW, data=carnivora, geom="histogram",
            binwidth=25)
pBW <- qplot(BW, data=carnivora, geom="histogram",
            binwidth=100, na.rm=TRUE)
@ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Transformations, motivation}
  Look at \texttt{SW}.
<<fig.width=2.5, fig.height=2.5, fig.align="center">>=
pSW
@ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Transformations, motivation}
  Check out \texttt{BW}.
<<fig.width=2.5, fig.height=2.5, fig.align="center">>=
pBW
@ 
\end{frame}

\begin{frame}
  \frametitle{Transformations, motivation}
  Is there anything we can do to these variables to ``encourage'' them to be more linearly related?  Yes, we can transform the variables with some function before investigating them.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Transformations, motivation}
Consider our data set with transformed variables, $x = \log(BW)$ and $y = \log(SW)$.
<<>>=
p2 <- qplot(log(BW), log(SW), data=carnivora,
          na.rm=TRUE, xlab="Birth weight (log(g))",
          ylab="Body weight (log(kg))")
@ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Transformations, motivation}
Isn't the relationship here more linear?
<<echo=FALSE, fig.height=3, fig.width=3, fig.align="center">>=
p2
@ 
\end{frame}


\begin{frame}[fragile]
  \frametitle{Transformations, motivation}
Now there is one potential outlier -- let's remove the runt.
<<>>=
suppressMessages(library(dplyr))
carn <- filter(carnivora, BW>0.01) # remove runt
p3 <- qplot(log(BW), log(SW), data=carn,
           na.rm=TRUE, xlab="Birth weight (log(g))",
           ylab="Body weight (log(kg))")
@ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Transformations, motivation}
Without that outlier, these data are ready for linear regression.
<<echo=FALSE, fig.height=3, fig.width=3, fig.align="center">>=
p3
@ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Transformations, motivation}
Let's fit simple linear regression to the transformed data.
<<>>=
fit <- lm(log(SW)\textasciitildelog(BW), data=carn)
# summary(fit) # RStudio
r <- rstandard(fit)
yhat <- fitted(fit)
@ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Transformations, motivation}
<<echo=FALSE, fig.height=3, fig.width=3, fig.align="center">>=
qplot(yhat, r) + geom_hline(yintercept=0)
@ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Transformations, motivation}
<<echo=FALSE, fig.height=3, fig.width=3, fig.align="center">>=
qplot(r, geom="histogram", binwidth=1/3)
@ 
\end{frame}

\section{Transformations}

\begin{frame}
  \frametitle{Transformations}
  There is a small set of common transformations:
  \begin{itemize}
  \item natural log -- $\log(x)$
  \item sqrt -- $\sqrt{x}$
  \item inverse -- $1/x$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Transformation, natural log}
  The natural log transformation is commonly used on data that are positive and right skewed.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Transformations, re \texttt{BW}}
Consider again the variable \texttt{BW} from \texttt{ape::carnivora}.
<<echo=FALSE, fig.height=2.25, fig.width=2.25, fig.align="center", fig.show="hold">>=
qplot(BW, data=carn, na.rm=TRUE, geom="histogram", binwidth=20)
qplot(log(BW), data=carn, na.rm=TRUE, geom="histogram", binwidth=2)
@ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Transformations, re \texttt{BW}}
Consider again the variable \texttt{SW} from \texttt{ape::carnivora}.
<<echo=FALSE, fig.height=2.25, fig.width=2.25, fig.align="center", fig.show="hold">>=
qplot(SW, data=carn, na.rm=TRUE, geom="histogram", binwidth=20)
qplot(log(SW), data=carn, na.rm=TRUE, geom="histogram", binwidth=2)
@ 
\end{frame}

\begin{frame}
  \frametitle{Transformation, sqrt}
  The sqrt transformation is commonly used on data that represent counts, or other positive values.
\end{frame}

\begin{frame}
  \frametitle{Transformation, inverse}
  The inverse transformation is commonly used on nonzero data, and often mainly to aid translation of the variable of interest.
\end{frame}

\begin{frame}
  \frametitle{Transformation, caution take I}
One should apply transformations thoughtfully.  These transformations have domains that are non-debatable.
  \begin{itemize}
  \item natural log -- positive numbers
  \item sqrt -- non-negative numbers
  \item inverse -- non-zero numbers
  \end{itemize}
\end{frame}

\subsection{Linearity?}

\begin{frame}
  \frametitle{non-linear tangent}
  The word linear in linear regression means linear in the coefficients $\beta$, not the explanatory variable(s).
\end{frame}

\begin{frame}
  \frametitle{Transformation, caution take II}

The fitted regression equations must be interpretted relative to their new variables.  For instance, if you use $\log(x)$ as the explanatory variable, then the intercept is the value the response variable takes on when $\log(x) = 0$, not when $x = 0$.  In fact, it is when $x = 1$ that the explanatory variable $\log(x) = 0$,

\begin{align*}
 \hat{y} & = \hat{\beta}_0 \\
         & = \hat{\beta}_0 + \hat{\beta}_1 \cdot 0 \\
         & = \hat{\beta}_0 + \hat{\beta}_1 \log(1) \\
\end{align*}
\end{frame}

\section{Tranformations Translations}

\begin{frame}
  \frametitle{Translation, linear regression}
  We already know how to translate slopes in standard linear regression.  In the equation
  \[ y = \beta_0 + \beta_1 x \]
  $y$ increases by $\beta_1$ units on average for every unit increase in $x$.
\end{frame}

\begin{frame}
  \frametitle{Translation, linear regression}
 If we transform $x$ into $\log(x)$, then in the equation
\[ y = \beta_0 + \beta_1 \log(x) \]
$y$ increases on average $\beta_1/100$ units for every $1$\% increase in $x$.
\end{frame}

\begin{frame}
  \frametitle{Translation, linear regression}
 If we transform $y$ into $\log(y)$, then in the equation
\[ \log(y) = \beta_0 + \beta_1 x \]
$y$ increases on average $100\beta_1\%$ units for every unit increase in $x$.
\end{frame}

\begin{frame}
  \frametitle{Translation, linear regression}
 If we transform $y$ into $\log(y)$ and $x$ into $\log(x)$, then in the equation
\[ \log(y) = \beta_0 + \beta_1 \log(x) \]
$y$ increases by $\beta_1\%$ for every $1\%$ increase in $x$.
\end{frame}



\begin{frame}
  \frametitle{Transformations, take away}

  \begin{itemize}
  \item Transformations can be applied, to better meet assumptions, for any analysis
    \begin{itemize}
    \item<2-> means and confidence intervals
    \item<3-> linear regression
    \item<4-> $\ldots$
    \end{itemize}    
  \item<5-> Apply transformations with forethought
  \item<6-> Translate the transformed variables appropriately
  \end{itemize}

\end{frame}


\section{References}
\nocite{Diez:2015}
\begin{frame}[allowframebreaks]
  \frametitle{references}
  \bibliographystyle{plainnat} \bibliography{../ref}
\end{frame}

\end{document}
